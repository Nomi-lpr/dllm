# -*- coding: utf-8 -*-
from operator import truediv
import torch
import numpy as np
import torch.nn.functional as F
from typing import Optional, Union, List, Dict, Any
from transformers import AutoTokenizer, AutoModel
import accelerate
from tqdm import tqdm
import torch
from pathlib import Path
from collections import defaultdict
import textwrap
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore') 

import os
# å¯¼å…¥å¯è§†åŒ–å‡½æ•°

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False



def add_gumbel_noise(logits: torch.Tensor, temperature: float) -> torch.Tensor:
    """
    Gumbel maxé‡‡æ ·æ–¹æ³•ï¼Œç”¨äºåˆ†ç±»åˆ†å¸ƒé‡‡æ ·
    æ ¹æ®arXiv:2409.02908ï¼Œå¯¹äºMDMï¼Œä½ç²¾åº¦Gumbel Maxæé«˜å›°æƒ‘åº¦åˆ†æ•°ä½†é™ä½ç”Ÿæˆè´¨é‡
    å› æ­¤ä½¿ç”¨float64
    
    Args:
        logits: æ¨¡å‹è¾“å‡ºçš„logits
        temperature: é‡‡æ ·æ¸©åº¦
        
    Returns:
        æ·»åŠ Gumbelå™ªå£°åçš„logits
    """
    if temperature == 0:
        return logits
    logits = logits.to(torch.float64)
    noise = torch.rand_like(logits, dtype=torch.float64)
    gumbel_noise = (- torch.log(noise)) ** temperature
    return logits.exp() / gumbel_noise

def get_num_transfer_tokens(mask_index: torch.Tensor, steps: int) -> torch.Tensor:
    """
    åœ¨åå‘è¿‡ç¨‹ä¸­ï¼ŒåŒºé—´[0,1]è¢«å‡åŒ€ç¦»æ•£åŒ–ä¸ºstepsä¸ªåŒºé—´
    ç”±äºLLaDAé‡‡ç”¨çº¿æ€§å™ªå£°è°ƒåº¦ï¼ˆå¦‚Eq.(8)å®šä¹‰ï¼‰ï¼Œ
    æ¯æ­¥é¢„æœŸçš„tokenè½¬ç§»æ•°é‡åº”è¯¥æ˜¯ä¸€è‡´çš„
    
    æ­¤å‡½æ•°é¢„è®¡ç®—æ¯æ­¥éœ€è¦è½¬ç§»çš„tokenæ•°é‡
    
    Args:
        mask_index: æ©ç ç´¢å¼•å¼ é‡
        steps: é‡‡æ ·æ­¥æ•°
        
    Returns:
        æ¯æ­¥è½¬ç§»tokenæ•°é‡çš„å¼ é‡
    """
    mask_num = mask_index.sum(dim=1, keepdim=True)
    
    base = mask_num // steps
    remainder = mask_num % steps
    
    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base
    
    for i in range(mask_num.size(0)):
        num_transfer_tokens[i, :remainder[i]] += 1
    
    return num_transfer_tokens

#è®¡ç®—IEAR(AR ç‰ˆæœ¬å’ŒNAR ç‰ˆæœ¬)
def calculate_iear_metrics(
    attentions: tuple[torch.Tensor],
    current_sequence_ids: torch.Tensor,
    tokenizer: AutoTokenizer,
    # mask_id: int,
    gen_start:int, #é™åˆ¶æŒ‡æ ‡è®¡ç®—åŒºåŸŸ
    gen_length: int,  # æ–°å¢å‚æ•°
    split_label: str = "\n\n"  # ä½¿ç”¨\n\nä½œä¸ºICEåˆ†éš”ç¬¦
) -> Optional[Dict]:
    """
    è®¡ç®—æ”¹è¿›ç‰ˆçš„ IEAR æŒ‡æ ‡ã€‚
    
    åŒæ—¶å…³æ³¨ï¼š
    1. Individual-level (ç‚¹å¯¹ç‚¹): æ¯ä¸ªCurrent ICE tokenä»Previous/Currentè·å¾—çš„å¹³å‡æ³¨æ„åŠ›
    2. Collective-level (æ•´ä½“): Previous/Current ICEä½œä¸ºæ•´ä½“å¯¹Current ICEçš„æ€»ä½“å½±å“åŠ›
    
    Args:
        attentions: æ¨¡å‹æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›æƒé‡å…ƒç»„ï¼Œå½¢çŠ¶ä¸º [B, H, S, S]
        current_sequence_ids: å½“å‰æ‰¹æ¬¡çš„token IDå¼ é‡ï¼Œå½¢çŠ¶ä¸º [B, S]
        tokenizer: ç”¨äºè§£ç çš„åˆ†è¯å™¨
        mask_id: mask tokençš„IDï¼Œç”¨äºè¯†åˆ«ç”Ÿæˆéƒ¨åˆ†
        split_label: ç”¨äºåˆ†éš”ICEçš„å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º"\n\n"
    
    Returns:
        åŒ…å«å¤šå±‚æ¬¡IEARæŒ‡æ ‡çš„å­—å…¸ï¼Œæˆ–åœ¨ä¸é€‚ç”¨æ—¶è¿”å›None
    """
    if attentions is None or len(attentions) == 0:
        return None

    # å½“å‰åºåˆ—ï¼ˆå»æ‰ batch ç»´ï¼‰
    seq_ids = current_sequence_ids[0]  # é•¿åº¦ S
    S = len(seq_ids)

    # 1. æ‰¾åˆ°anchor tokençš„ä½ç½®ï¼ˆéœ€è¦æ’é™¤çš„ç‰¹æ®Štokenï¼‰
    decoded = [tokenizer.decode([tid]) for tid in seq_ids]
    anchor_chars = {'.', '\n'}
    anchor_idx = {i for i, s in enumerate(decoded) if (s.strip() == '.' or s == '\n')}

    # 1. æ‰¾åˆ°ICEçš„è¾¹ç•Œï¼ˆä½¿ç”¨\n\nä½œä¸ºåˆ†éš”ç¬¦ï¼‰
    # è·å–split_labelçš„token IDs
    split_token_ids = tokenizer.encode(split_label, add_special_tokens=False)
    split_positions = []
    gen_end = gen_start + gen_length
    
    # è¿™é‡Œå¯ä»¥è¿›è¡Œæ”¹è¿›
    # æ‰¾åˆ°æ‰€æœ‰åˆ†éš”ç¬¦çš„ä½ç½®
    # split_positions = []
    for i in range(S - len(split_token_ids) + 1):
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…åˆ†éš”ç¬¦åºåˆ—
        if all(seq_ids[i + j] == split_token_ids[j] for j in range(len(split_token_ids))):
            # æ’é™¤ç”ŸæˆåŒºåŸŸå†…çš„åˆ†éš”ç¬¦
            if not (gen_start <= i < gen_end):
                split_positions.append(i + len(split_token_ids) - 1)  # è®°å½•åˆ†éš”ç¬¦çš„æœ€åä¸€ä¸ªä½ç½®

            # else:
            #     # ğŸ” è°ƒè¯•2ï¼šæ‰“å°è¢«æ’é™¤çš„åˆ†éš”ç¬¦
            #     print(f"[IEARè°ƒè¯•] æ’é™¤ç”ŸæˆåŒºåŸŸå†…çš„åˆ†éš”ç¬¦: position {i}")

    # ğŸ” è°ƒè¯•3ï¼šæ‰“å°æ‰¾åˆ°çš„åˆ†éš”ç¬¦ä½ç½®
    # print(f"[IEARè°ƒè¯•] æ‰¾åˆ° {len(split_positions)} ä¸ªæœ‰æ•ˆåˆ†éš”ç¬¦: {split_positions}")

    # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œæ— æ³•è®¡ç®—IEAR
    if len(split_positions) < 2:
        # print(f"[IEARè°ƒè¯•] âŒ åˆ†éš”ç¬¦ä¸è¶³2ä¸ªï¼Œè¿”å›None")
        return None

    # 3. å®šä¹‰ICEè¾¹ç•Œ
    # ICEè¾¹ç•Œ: [0, split_pos[0]], [split_pos[0]+1, split_pos[1]], ..., [split_pos[-2]+1, split_pos[-1]]
    # æœ€åä¸€ä¸ªåˆ†éš”ç¬¦ä¹‹ååˆ°generation_startä¹‹å‰çš„æ˜¯æœ€åçš„queryï¼ˆéœ€è¦æ’é™¤ï¼‰
    ice_boundaries = []

    # ç¬¬ä¸€ä¸ªICE: ä»åºåˆ—å¼€å§‹åˆ°ç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦
    if split_positions[0] > 2:  # ç¡®ä¿ICEæœ‰è¶³å¤Ÿçš„token
        ice_boundaries.append((0, split_positions[0]))

    # ä¸­é—´çš„ICE: æ¯ä¸¤ä¸ªåˆ†éš”ç¬¦ä¹‹é—´
    for i in range(len(split_positions) - 1):
        start = split_positions[i] + 1
        end = split_positions[i + 1]
        # ç¡®ä¿ICEä¸ä¸ç”ŸæˆåŒºåŸŸé‡å 
        if end < gen_start or start >= gen_end:
            ice_boundaries.append((start, end))

        # else:
        #     # ğŸ” è°ƒè¯•4ï¼šæ‰“å°è¢«æ’é™¤çš„ICE
        #     print(f"[IEARè°ƒè¯•] æ’é™¤ä¸ç”ŸæˆåŒºåŸŸé‡å çš„ICE: [{start}, {end}]")

    # ğŸ”§ æ–°å¢ï¼šæœ€åä¸€ä¸ªICEï¼ˆä»æœ€åä¸€ä¸ªåˆ†éš”ç¬¦åˆ°åºåˆ—æœ«å°¾ï¼‰
    if len(split_positions) > 0:
        last_start = split_positions[-1] + 1
        last_end = S - 1  # åºåˆ—æœ«å°¾

        # åˆ¤æ–­æ˜¯å¦ä¸ç”ŸæˆåŒºåŸŸé‡å 
        if last_end < gen_start or last_start >= gen_end:
            # å®Œå…¨ä¸é‡å ï¼Œæ·»åŠ æ•´ä¸ªICE
            ice_boundaries.append((last_start, last_end))
            # print(f"[IEARè°ƒè¯•] æ·»åŠ æœ€åä¸€ä¸ªICE: [{last_start}, {last_end}]")
        elif last_start < gen_start <= last_end:
            # è¢«ç”ŸæˆåŒºåŸŸåˆ†å‰²ï¼Œåªä¿ç•™å‰åŠæ®µ
            ice_boundaries.append((last_start, gen_start - 1))
            # print(f"[IEARè°ƒè¯•] æ·»åŠ æœ€åä¸€ä¸ªICEçš„å‰åŠæ®µ: [{last_start}, {gen_start - 1}]")
        elif last_start < gen_end <= last_end:
            # è¢«ç”ŸæˆåŒºåŸŸåˆ†å‰²ï¼Œåªä¿ç•™ååŠæ®µ
            ice_boundaries.append((gen_end, last_end))
            # print(f"[IEARè°ƒè¯•] æ·»åŠ æœ€åä¸€ä¸ªICEçš„ååŠæ®µ: [{gen_end}, {last_end}]")
        # else:
        #     print(f"[IEARè°ƒè¯•] æœ€åä¸€ä¸ªICEå®Œå…¨åœ¨ç”ŸæˆåŒºåŸŸå†…ï¼Œè·³è¿‡: [{last_start}, {last_end}]")

    # ğŸ” è°ƒè¯•5ï¼šæ‰“å°æœ€ç»ˆçš„ICEè¾¹ç•Œ
    # print(f"[IEARè°ƒè¯•] è¯†åˆ«åˆ° {len(ice_boundaries)} ä¸ªæœ‰æ•ˆICE:")
    # for idx, (start, end) in enumerate(ice_boundaries):
    #     ice_text = tokenizer.decode(seq_ids[start:end+1])[:1000]  # åªæ˜¾ç¤ºå‰50ä¸ªå­—ç¬¦
    #     print(f"  ICE {idx}: [{start:4d}, {end:4d}] é•¿åº¦={end-start+1:3d} | å†…å®¹: {ice_text}...")
    

    
    if len(ice_boundaries)<2:
        # print(f"[IEARè°ƒè¯•] âŒ æœ‰æ•ˆICEä¸è¶³2ä¸ªï¼Œè¿”å›None")
        return None

    # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºè¯†åˆ«åˆ°çš„ICEæ•°é‡
    # print(f"[IEAR] è¯†åˆ«åˆ° {len(ice_boundaries)} ä¸ªICEï¼ˆä»…åœ¨ç”ŸæˆåŒºåŸŸä¹‹å‰ï¼Œgen_start={gen_start}ï¼‰")

    # 4. å¯¹æ¯ä¸ªICEï¼Œè®¡ç®—å…¶å†…éƒ¨tokenå¯¹intra/extraçš„æ³¨æ„åŠ›åˆ†é…
    num_layers = len(attentions)


    #é€å±‚å­˜å‚¨æŒ‡æ ‡
    # intra_attentions_per_layer:List[float] = []#æ¯å±‚æ‰€æœ‰ICEçš„å¹³å‡intraæ³¨æ„åŠ›
    # extra_attentions_per_layer:List[float] = []#æ¯å±‚æ‰€æœ‰ICEçš„å¹³å‡extraæ³¨æ„åŠ›
    iear_ratio_per_layer:List[float] = []#æ¯å±‚IEARçš„æ•´ä½“æ¯”ç‡
    iear_ratio_per_layer_individual:List[float] = []#æ¯å±‚æ‰€æœ‰ICEçš„å¹³å‡ä¸ªä½“æ³¨æ„åŠ›

    
    for l in range(num_layers):
        att_l=attentions[l] #[N,H,S,S]
        if att_l.dim() != 4 or att_l.size(-1) != S or att_l.size(-2) != S:
            continue

        #å¤´å¹³å‡æ³¨æ„åŠ›[Sï¼ŒS]
        ave_att=att_l[0].mean(dim=0)
        
        #å¯¹æ¯ä¸ªICEåˆ†åˆ«è®¡ç®—
        ice_intra_scores=[]
        ice_extra_scores=[]
        ice_intra_scores_individual=[]
        ice_extra_scores_individual=[]

        for ice_idx,(ice_start,ice_end) in enumerate(ice_boundaries):
            #å½“å‰ICEçš„tokenç´¢å¼•
            current_ice_tokens=set(range(ice_start,ice_end+1))

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            # print(f"ICE {ice_idx}: åŸå§‹èŒƒå›´ [{ice_start}, {ice_end}], åŸå§‹tokenæ•°: {len(current_ice_tokens)}")


            current_ice_tokens = sorted(current_ice_tokens.difference(anchor_idx))
            
            # æ·»åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯ - ä¿®å¤ç±»å‹é”™è¯¯
            # original_tokens = set(range(ice_start, ice_end+1))
            # anchor_tokens = original_tokens - set(current_ice_tokens)
            # print(f"ICE {ice_idx}: è¿‡æ»¤anchoråtokenæ•°: {len(current_ice_tokens)}")
            # print(f"ICE {ice_idx}: anchor tokenæ•°: {len(anchor_tokens)}")

            #å…¶ä»–ICEçš„tokenç´¢å¼•
            other_ice_tokens=set()
            for other_idx,(other_start,other_end) in enumerate(ice_boundaries):
                if other_idx !=ice_idx:
                    other_ice_tokens.update(range(other_start,other_end+1))

            other_ice_tokens=sorted(other_ice_tokens.difference(anchor_idx))

            #è½¬ä¸ºå¼ é‡
            current_tensor=torch.tensor(current_ice_tokens, device=ave_att.device)
            other_tensor=torch.tensor(other_ice_tokens, device=ave_att.device)

            #æå–å½“å‰ICE tokençš„æ³¨æ„åŠ›
            current_ice_att=ave_att.index_select(dim=0,index=current_tensor)

            #è®¡ç®—æ€»æµå…¥
            total_inflow=current_ice_att.sum(dim=1)  # [|current_valid|]
            total_inflow=torch.where(total_inflow==0,torch.ones_like(total_inflow),total_inflow)#é˜²æ­¢éƒ½ä¸º0

            #Intra-ICEï¼šå½“å‰ICEå†…éƒ¨æœ‰æ•ˆtokenä¹‹é—´çš„æ³¨æ„åŠ›
            intra_att=current_ice_att.index_select(dim=1,index=current_tensor)
            intra_score=(intra_att.sum(dim=1)/total_inflow).mean().item()

            #Extra-ICE:å½“å‰ICEå¯¹å…¶ä»–ICEæœ‰æ•ˆtokençš„æ³¨æ„åŠ›
            extra_att=current_ice_att.index_select(dim=1,index=other_tensor)   # [|current|, |current|]
            extra_score=(extra_att.sum(dim=1)/total_inflow).mean().item()
            
            # print("len(current_ice_tokens):",len(current_ice_tokens))
            # print("len(other_ice_tokens):",len(other_ice_tokens))

            #è®¡ç®—ä¸ªä½“æŒ‡æ ‡
            intra_score_individual=intra_score/len(current_ice_tokens)
            extra_score_individual=extra_score/len(other_ice_tokens)

            ice_intra_scores.append(intra_score)
            ice_extra_scores.append(extra_score)
            ice_intra_scores_individual.append(intra_score_individual)
            ice_extra_scores_individual.append(extra_score_individual)

        #è®¡ç®—è¯¥å±‚æ‰€æœ‰ICEçš„å¹³å‡
        #åŒç†ï¼Œè¿™ä¸ªä¹Ÿæ˜¯æ•´ä½“ï¼Œè¿˜æ˜¯éœ€è¦å»çœ‹ä¸ªä½“æŒ‡æ ‡
        if len(ice_intra_scores)>0:
            #è¿™ä¸ªæ˜¯æ•´ä½“
            avg_intra = np.mean(ice_intra_scores)
            avg_extra=np.mean(ice_extra_scores)
            #è¿™ä¸ªæ˜¯ä¸ªä½“
            avg_intra_individual=np.mean(ice_intra_scores_individual)
            avg_extra_individual=np.mean(ice_extra_scores_individual)
            #è®¡ç®—ä¸¤ç§æŒ‡æ ‡çš„æ¯”ç‡
            ratio_collective = avg_intra / avg_extra if avg_extra >1e-9 else 0.0
            ratio_individual = avg_intra_individual / avg_extra_individual if avg_extra_individual >1e-9 else 0.0

            # intra_attentions_per_layer.append(intra_score)
            # extra_attentions_per_layer.append(extra_score)
            iear_ratio_per_layer.append(ratio_collective)
            iear_ratio_per_layer_individual.append(ratio_individual)
        else:
            # intra_attentions_per_layer.append(0.0)
            # extra_attentions_per_layer.append(0.0)
            iear_ratio_per_layer.append(0.0)
            iear_ratio_per_layer_individual.append(0.0)

    return {
        #ä¸»è¦æŒ‡æ ‡
        # "intra_attentions_per_layer": intra_attentions_per_layer,
        # "extra_attentions_per_layer": extra_attentions_per_layer,
        "iear_ratio_per_layer": iear_ratio_per_layer,#æ•´ä½“æ¯”ç‡
        "iear_ratio_per_layer_individual": iear_ratio_per_layer_individual,#ä¸ªä½“æ¯”ç‡
    }


def plot_iear_metrics_per_step(
    all_step_metrics:List[dict],
    # all_decoded_texts:List[str], 
    save_dir:str="IEAR_analysis_per_step"):
    """
    ä¸ºæ¯ä¸ªå»å™ªæ­¥éª¤ç”ŸæˆIEARæŒ‡æ ‡å›¾ï¼ŒåŒ…å«ä¸¤ä¸ªå­å›¾ï¼š
    - å·¦å›¾ï¼šIndividual-level IEAR (ç‚¹å¯¹ç‚¹å¹³å‡æ³¨æ„åŠ›æ¯”ç‡)
    - å³å›¾ï¼šCollective-level IEAR (æ•´ä½“å½±å“åŠ›æ¯”ç‡)
    
    Args:
        all_step_metrics: åŒ…å«æ¯ä¸€æ­¥IEARæŒ‡æ ‡å­—å…¸çš„åˆ—è¡¨
        all_decoded_texts: åŒ…å«æ¯ä¸€æ­¥è§£ç æ–‡æœ¬çš„åˆ—è¡¨
        save_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
    """
    # --- 1. é¢„å¤„ç†å’Œæ£€æŸ¥ ---
    if not all_step_metrics:
        print("æ²¡æœ‰å¯ä¾›å¯è§†åŒ–çš„IEARæŒ‡æ ‡ã€‚")
        return

    # è¿‡æ»¤æ‰æ— æ•ˆçš„æ­¥éª¤æ•°æ®
    valid_metrics = [m for m in all_step_metrics if m and "iear_ratio_per_layer_individual" in m and "iear_ratio_per_layer" in m]
    if not valid_metrics:
        print("æ‰€æœ‰æ­¥éª¤å‡æ— å¯ä¾›å¯è§†åŒ–çš„IEARæŒ‡æ ‡ã€‚")
        return

    # --- 2. åˆ›å»ºä¿å­˜å›¾ç‰‡çš„ç›®å½• ---
    save_directory = Path(save_dir)
    save_directory.mkdir(parents=True, exist_ok=True)
    print(f"å¼€å§‹ç”Ÿæˆ {len(valid_metrics)} å¼ IEARåˆ†æå›¾ï¼Œå°†ä¿å­˜è‡³ '{save_directory}' ç›®å½•...")

    # --- 3. éå†æ¯ä¸€æ­¥ï¼Œç”Ÿæˆä¸€å¼ å›¾ ---
    for step_idx, metrics in tqdm(enumerate(valid_metrics), total=len(valid_metrics), desc="ç”ŸæˆIEARåˆ†æå›¾ä¸­"):
        # æå–å½“å‰æ­¥éª¤çš„ä¸¤ç§IEARæŒ‡æ ‡æ•°æ®
        iear_individual = metrics["iear_ratio_per_layer_individual"]
        iear_collective = metrics["iear_ratio_per_layer"]

        num_layers = len(iear_individual)
        layers_x_axis = range(num_layers)

        # --- 4. åˆ›å»ºåŒ…å«ä¸¤ä¸ªå­å›¾çš„ç”»å¸ƒï¼ˆ1è¡Œ2åˆ—ï¼‰ ---
        fig,(ax1,ax2) =plt.subplots(1,2,figsize=(16,6))

        # ä¸ºæ•´å¼ å›¾è®¾ç½®ä¸€ä¸ªæ€»æ ‡é¢˜
        fig.suptitle(f'IEAR æŒ‡æ ‡åˆ†æ (å»å™ªæ­¥éª¤ Step {step_idx})', fontsize=16, fontweight='bold')

        # --- 5. ç»˜åˆ¶å·¦å­å›¾ï¼šIndividual-level IEAR ---
        ax1.plot(layers_x_axis, iear_individual, marker='o', linestyle='-', color='darkorange', label='Individual IEAR', linewidth=2)
        # æ·»åŠ ç½®ä¿¡åŒºé—´å¼çš„å¡«å……
        avg_val = np.mean(iear_individual)
        std_val = np.std(iear_individual)
        ax1.fill_between(layers_x_axis, 
                         np.array(iear_individual) - std_val, 
                         np.array(iear_individual) + std_val, 
                         color='darkorange', alpha=0.2)
        ax1.set_title('Individual-level IEAR (ç‚¹å¯¹ç‚¹å¹³å‡æ¯”ç‡)', fontsize=13, fontweight='bold')
        ax1.set_xlabel('Layer Number', fontsize=12)
        ax1.set_ylabel('IEAR Individual Value', fontsize=12)
        ax1.grid(True, linestyle=':', alpha=0.6)
        ax1.legend(fontsize=11)

               # --- 6. ç»˜åˆ¶å³å­å›¾ï¼šCollective-level IEAR ---
        ax2.plot(layers_x_axis, iear_collective, marker='s', linestyle='-', color='dodgerblue', label='Collective IEAR', linewidth=2)
        avg_val_2 = np.mean(iear_collective)
        std_val_2 = np.std(iear_collective)
        ax2.fill_between(layers_x_axis, 
                         np.array(iear_collective) - std_val_2, 
                         np.array(iear_collective) + std_val_2, 
                         color='dodgerblue', alpha=0.2)
        ax2.set_title('Collective-level IEAR (æ•´ä½“å½±å“åŠ›æ¯”ç‡)', fontsize=13, fontweight='bold')
        ax2.set_xlabel('Layer Number', fontsize=12)
        ax2.set_ylabel('IEAR Collective Value', fontsize=12)
        ax2.grid(True, linestyle=':', alpha=0.6)
        ax2.legend(fontsize=11)

                # --- 7. è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜ ---
        plt.tight_layout(rect=[0, 0, 1, 0.96])  # ä¸ºæ€»æ ‡é¢˜ç•™å‡ºç©ºé—´
        filename = save_directory / f"step_{step_idx:03d}_iear_analysis.png"
        plt.savefig(filename, dpi=120, bbox_inches='tight')
        plt.close(fig)  # å…³é”®ï¼šåœ¨å¾ªç¯ä¸­å…³é—­ç”»å¸ƒï¼Œé˜²æ­¢å†…å­˜æ³„æ¼

    # å¾ªç¯ç»“æŸåç»Ÿä¸€è¾“å‡º
    print(f"æ‰€æœ‰ {len(valid_metrics)} å¼ IEARåˆ†æå›¾å·²æˆåŠŸä¿å­˜ã€‚")


# def plot_metrics_per_step(all_step_metrics:List[dict], all_decoded_texts:List[str], save_dir:str="ACAR_analysis_per_step"):
#     """
#     ä¿®æ”¹åçš„å¯è§†åŒ–å‡½æ•°ï¼š
#     ä¸ºæ¯ä¸ªå»å™ªæ­¥éª¤ç”Ÿæˆä¸€å¼ å›¾ï¼Œå›¾ä¸­åŒ…å«ä¸¤ä¸ªå­å›¾ï¼Œ
#     åˆ†åˆ«å±•ç¤ºä¸¤ç§ACARæŒ‡æ ‡éšâ€œå±‚æ•°â€çš„å˜åŒ–ã€‚
#     åŒæ—¶ï¼Œæˆ‘ç°åœ¨æƒ³çœ‹åˆ°çš„æ˜¯æ¯ä¸€æ­¥çš„è§£ç è¿‡ç¨‹
#     Args:
#         all_step_metrics: åŒ…å«æ¯ä¸€æ­¥æŒ‡æ ‡å­—å…¸çš„åˆ—è¡¨ã€‚
#         save_path: å›¾ç‰‡ä¿å­˜è·¯å¾„ã€‚
#     """
#     # --- 1. é¢„å¤„ç†å’Œæ£€æŸ¥ ---
#     if not all_step_metrics:
#         print("æ²¡æœ‰å¯ä¾›å¯è§†åŒ–çš„æŒ‡æ ‡ã€‚")
#         return

#     # è¿‡æ»¤æ‰æ— æ•ˆçš„æ­¥éª¤æ•°æ®
#     valid_metrics = [m for m in all_step_metrics if m and "ratio_scaled_avg" in m and "ratio_collective" in m]
#     if not valid_metrics:
#         print("æ‰€æœ‰æ­¥éª¤å‡æ— å¯ä¾›å¯è§†åŒ–çš„æŒ‡æ ‡ã€‚")
#         return

#     # --- 2. åˆ›å»ºä¿å­˜å›¾ç‰‡çš„ç›®å½• ---
#     save_directory = Path(save_dir)
#     save_directory.mkdir(parents=True, exist_ok=True)
#     print(f"å¼€å§‹ç”Ÿæˆ {len(valid_metrics)} å¼ åˆ†æå›¾ï¼Œå°†ä¿å­˜è‡³ '{save_directory}' ç›®å½•...")

#     # --- 3. éå†æ¯ä¸€æ­¥ï¼Œç”Ÿæˆä¸€å¼ å›¾ ---
#     for step_idx, metrics in tqdm(enumerate(valid_metrics), total=len(valid_metrics), desc="ç”Ÿæˆåˆ†æå›¾ä¸­"):
        
#         # æå–å½“å‰æ­¥éª¤çš„ä¸¤ç§æŒ‡æ ‡æ•°æ®
#         ratio_scaled_avg = metrics["ratio_scaled_avg"]
#         ratio_collective = metrics["ratio_collective"]

#         num_layers = len(ratio_scaled_avg)
#         layers_x_axis = range(num_layers)


#         # # --- 4. åˆ›å»ºåŒ…å«ä¸¤ä¸ªå­å›¾çš„ç”»å¸ƒ ---
#         # fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))

#         # --- 4. åˆ›å»ºåŒ…å«ä¸¤ä¸ªå­å›¾å’Œæ–‡æœ¬åŒºåŸŸçš„ç”»å¸ƒ ---
#         fig = plt.figure(figsize=(18, 9))
        
#         # åˆ›å»ºå­å›¾å¸ƒå±€ï¼šä¸Šæ–¹ä¸¤ä¸ªå›¾è¡¨ï¼Œä¸‹æ–¹æ–‡æœ¬åŒºåŸŸ
#         gs = fig.add_gridspec(2, 2, height_ratios=[2, 2], hspace=0.3)
#         ax1 = fig.add_subplot(gs[0, 0])  # å·¦ä¸Š
#         ax2 = fig.add_subplot(gs[0, 1])  # å³ä¸Š
#         ax_text = fig.add_subplot(gs[1, :])  # ä¸‹æ–¹è·¨ä¸¤åˆ—

#         # ä¸ºæ•´å¼ å›¾è®¾ç½®ä¸€ä¸ªæ€»æ ‡é¢˜
#         fig.suptitle(f'ACAR æŒ‡æ ‡åˆ†æ (å»å™ªæ­¥éª¤ Step {step_idx})', fontsize=16)
    

#         # --- 5. ç»˜åˆ¶å·¦å­å›¾ï¼šratio_scaled_avg ---
#         ax1.plot(layers_x_axis, ratio_scaled_avg, marker='o', linestyle='-', color='darkorange', label='ACAR Value')
#         # æ¨¡ä»¿ä½ æä¾›çš„å›¾ç‰‡é£æ ¼ï¼Œæ·»åŠ ç½®ä¿¡åŒºé—´å¼çš„å¡«å……
#         avg_val = np.mean(ratio_scaled_avg)
#         std_val = np.std(ratio_scaled_avg)
#         ax1.fill_between(layers_x_axis, 
#                          np.array(ratio_scaled_avg) - std_val, 
#                          np.array(ratio_scaled_avg) + std_val, 
#                          color='darkorange', alpha=0.2)
#         ax1.set_title('æŒ‡æ ‡ä¸€: Scaled Average Ratio')
#         ax1.set_xlabel('Number of Layers')
#         ax1.set_ylabel('ACAR_avg Value')
#         ax1.grid(True, linestyle=':', alpha=0.6)
#         ax1.legend()

#         # --- 6. ç»˜åˆ¶å³å­å›¾ï¼šratio_collective ---
#         ax2.plot(layers_x_axis, ratio_collective, marker='s', linestyle='-', color='dodgerblue', label='ACAR Value')
#         avg_val_2 = np.mean(ratio_collective)
#         std_val_2 = np.std(ratio_collective)
#         ax2.fill_between(layers_x_axis, 
#                          np.array(ratio_collective) - std_val_2, 
#                          np.array(ratio_collective) + std_val_2, 
#                          color='dodgerblue', alpha=0.2)
#         ax2.set_title('æŒ‡æ ‡äºŒ: Collective Influence Ratio')
#         ax2.set_xlabel('Number of Layers')
#         ax2.set_ylabel('ACAR_col Value')
#         ax2.grid(True, linestyle=':', alpha=0.6)
#         ax2.legend()

#         #å°†è§£ç çš„è¿‡ç¨‹æ ‡æ³¨åœ¨ä¸‹æ–¹
#         ax_text.axis('off')  # éšè—åæ ‡è½´
#         if step_idx < len(all_decoded_texts):
#             decoded_text = all_decoded_texts[step_idx]
#             # å¯¹è¶…é•¿æ–‡æœ¬è¿›è¡Œæ¢è¡Œä¸æˆªæ–­ï¼Œé¿å…ç»˜å›¾æ—¶å­—å½¢æ …æ ¼æº¢å‡º
#             max_chars = 6000  # æœ€å¤§å±•ç¤ºå­—ç¬¦æ•°
#             if len(decoded_text) > max_chars:
#                 decoded_text = decoded_text[:max_chars] + "\n...[æˆªæ–­]"
#             # é¿å…matplotlibæŠŠ$å½“ä½œmathtextè§£æå¯¼è‡´æŠ¥é”™ï¼šè½¬ä¹‰æ‰€æœ‰$
#             decoded_text = decoded_text.replace("$", r"\$")
#             wrapped = textwrap.fill(decoded_text, width=160)
#             ax_text.text(
#                 0.02,
#                 0.5,
#                 f"Step {step_idx} è§£ç ç»“æœ:\n{wrapped}",
#                 fontsize=9,
#                 verticalalignment='center',
#                 bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8),
#                 wrap=True,
#                 clip_on=True,
#             )
        
#         else:
#             ax_text.text(0.02, 0.5, f"Step {step_idx} è§£ç ç»“æœ: æ— æ•°æ®", 
#                 fontsize=10, verticalalignment='center',
#                 bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))     

#         # --- 7. ä¿å­˜å¹¶å…³é—­å½“å‰ç”»å¸ƒ ---
#         # plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # è°ƒæ•´å¸ƒå±€ä¸ºæ€»æ ‡é¢˜ç•™å‡ºç©ºé—´
        
#         # # ä½¿ç”¨è¡¥é›¶å‘½åï¼Œæ–¹ä¾¿æ–‡ä»¶æ’åºï¼Œä¾‹å¦‚ step_001.png
#         # filename = save_directory / f"step_{step_idx:03d}_analysis.png"
#         # plt.savefig(filename, dpi=120) # ä½¿ç”¨é€‚ä¸­çš„DPIä»¥å¹³è¡¡æ¸…æ™°åº¦å’Œæ–‡ä»¶å¤§å°


#         # --- 8. ä¿å­˜å¹¶å…³é—­å½“å‰ç”»å¸ƒ ---
#         filename = save_directory / f"step_{step_idx:03d}_analysis.png"
#         plt.savefig(filename, dpi=96, bbox_inches='tight')
#         plt.close(fig) # å…³é”®ï¼šåœ¨å¾ªç¯ä¸­å…³é—­ç”»å¸ƒï¼Œé˜²æ­¢å†…å­˜æ³„æ¼

#     # å¾ªç¯ç»“æŸåç»Ÿä¸€è¾“å‡ºä¸ç”Ÿæˆçƒ­å›¾
#     print(f"æ‰€æœ‰ {len(valid_metrics)} å¼ åˆ†æå›¾å·²æˆåŠŸä¿å­˜ã€‚")
#     # if confidence_matrix:
#     #     print("\n[åˆ†æ] ç”Ÿæˆç½®ä¿¡åº¦çƒ­å›¾...")
#     #     target_heatmap_dir = heatmap_dir if heatmap_dir else (save_dir + "_heatmap")
#     #     create_decode_heatmap(confidence_matrix, gen_length=256, save_dir=target_heatmap_dir)

#         # å¾ªç¯å†…ä¸å†ç”Ÿæˆçƒ­åŠ›å›¾ï¼Œä¹Ÿä¸é€æ­¥æ‰“å°â€œå·²ä¿å­˜â€æ±‡æ€»


    

#éœ€è¦è¿›è¡Œæ”¹åŠ¨ï¼Œå› ä¸ºæˆ‘è¿™ä¸ªgenerateæ˜¯é’ˆå¯¹æˆ‘è¿™ä¸ªpromptçš„ï¼Œè€Œä¸æ˜¯é’ˆå¯¹åŸå§‹çš„promptï¼Œä¹Ÿå°±æ˜¯è¯´queryä¸€ç›´åœ¨å˜åŒ–
@torch.no_grad()
def generate(
    model: AutoModel,
    prompt: torch.Tensor,
    gen_start: int,
    steps: int = 1024,
    gen_length: int = 1024,
    block_length: int = 1024,
    temperature: float = 0.0,
    cfg_scale: float = 0.0,
    remasking: str = 'low_confidence',
    mask_id: int = 126336,
    output_attentions: bool = False,  # æ–°å¢å‚æ•°
    IEAR_analyse: bool = False,
    sample_idx: int = None,  # æ–°å¢å‚æ•°
    query_position: int = 0  # æ–°å¢å‚æ•°
) -> Union[torch.Tensor, tuple[torch.Tensor, List]]:
    """
    LLaDAç”Ÿæˆå‡½æ•°ï¼Œä¸åŸå§‹ä»“åº“ä»£ç å®Œå…¨å¯¹é½
    
    Args:
        model: LLaDAæ¨¡å‹
        prompt: è¾“å…¥æç¤ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º(1, L)
        steps: é‡‡æ ·æ­¥æ•°ï¼Œå°äºç­‰äºgen_length
        gen_length: ç”Ÿæˆç­”æ¡ˆé•¿åº¦
        block_length: å—é•¿åº¦ï¼Œå°äºç­‰äºgen_lengthã€‚å¦‚æœå°äºgen_lengthï¼Œè¡¨ç¤ºä½¿ç”¨åŠè‡ªå›å½’é‡æ©ç 
        temperature: åˆ†ç±»åˆ†å¸ƒé‡‡æ ·æ¸©åº¦
        cfg_scale: æ— ç›‘ç£åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç¼©æ”¾
        remasking: é‡æ©ç ç­–ç•¥ã€‚'low_confidence'æˆ–'random'
        mask_id: [MASK]çš„token idï¼Œé»˜è®¤ä¸º126336
        
    Returns:
        ç”Ÿæˆçš„åºåˆ—å¼ é‡
    """
    # prompt æ­¤æ—¶åº”æ˜¯â€œå·²å±•å¼€â€çš„åºåˆ—ï¼š
    # - ä¸­é—´å¡«å……ï¼šprefix + [MASK]*gen_length + suffix
    # - å°¾éƒ¨è¡¥å…¨ï¼šinput_ids + [MASK]*gen_length
    # è¿™é‡Œä¸å†è¿½åŠ  maskï¼Œè€Œæ˜¯ç›´æ¥åœ¨ä¼ å…¥çš„ prompt ä¸Šæ“ä½œ



    x = prompt.clone().to(model.device)
    
    prompt_index = (x != mask_id)


    # ä»…åœ¨éœ€è¦æ—¶åˆå§‹åŒ–åˆ—è¡¨
    trigger_analysis = output_attentions and IEAR_analyse
    all_step_metrics = [] if trigger_analysis else None
    #å¢åŠ è§£ç çš„æ¶æ„ï¼Œæ–¹ä¾¿äººå·¥æ ‡æ³¨è§£ç çš„å½“ä¸‹æ­¥éª¤ï¼Œæ›´å¤šçš„æ˜¯å…³æ³¨ä»€ä¹ˆ
    # all_decoded_texts = [] if trigger_analysis else None  # æ–°å¢è¿™è¡Œ
    # ä¸å†è¿½è¸ªæ¯æ­¥è§£ç çš„ä½ç½®
    # confidence_matrix=[] if trigger_analysis else None#è¿½è¸ªæ¯æ­¥ç”ŸæˆåŒºåŸŸçš„ç½®ä¿¡åº¦
    
    # å¯¹äºéåŠè‡ªå›å½’æ¶æ„ï¼Œblock_lengthåº”è¯¥ç­‰äºgen_length
    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length
    
    assert steps % num_blocks == 0
    steps = steps // num_blocks
    


    for num_block in range(num_blocks):
# å–å½“å‰å—ä¸­ä»ä¸º mask çš„ä½ç½®
        block_mask_index = (
            x[:, gen_start + num_block * block_length : gen_start + (num_block + 1) * block_length] == mask_id
        )
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)
        
        #å…ˆåŠ å…¥è¿›åº¦æ¡ï¼Œæˆ‘è¦çœ‹ä¸€ä¸‹æ¯ä¸€æ­¥ä¸ºä»€ä¹ˆè§£ç è¿™ä¹ˆæ…¢
        progress_bar = tqdm(range(steps), desc=f"å»å™ª Block {num_block+1}/{num_blocks}", leave=False)
        #for i in range(steps):
        for i in progress_bar:
            mask_index = (x == mask_id)
            
            # åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼
            if cfg_scale > 0.:
                un_x = x.clone()
                un_x[prompt_index] = mask_id
                x_ = torch.cat([x, un_x], dim=0)
                outputs = model(x_, output_attentions=output_attentions)  # ä¿®æ”¹è¿™é‡Œ
                logits = outputs.logits
                logits, un_logits = torch.chunk(logits, 2, dim=0)
                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)

                if output_attentions and outputs.attentions:
                    cond_attentions = tuple(torch.chunk(att, 2, dim=0)[0] for att in outputs.attentions)
                    attentions_to_analyze = cond_attentions

            else:
                outputs=model(x,output_attentions=output_attentions)
                logits=outputs.logits
                if output_attentions and outputs.attentions:
                    attentions_to_analyze = outputs.attentions



            #æ¯ä¸€æ­¥éƒ½è®¡ç®—ï¼Œä½†æ˜¯æˆ‘ç°åœ¨æƒ³å¯¹æ¯ä¸€æ­¥ä¸­çš„æ¯ä¸€å±‚è¿›è¡Œå¤´å¹³å‡
            if trigger_analysis and attentions_to_analyze:
                # æ³¨æ„ï¼šå°†xå’Œattentionséƒ½ç§»åŠ¨åˆ°CPUè¿›è¡Œè®¡ç®—ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘GPUæ˜¾å­˜å³°å€¼
                metrics = calculate_iear_metrics(
                    attentions=tuple(att.cpu() for att in attentions_to_analyze), 
                    current_sequence_ids=x.cpu(), 
                    tokenizer=tokenizer, 
                    gen_start=gen_start,
                    gen_length=gen_length,  # ä¼ å…¥ç”Ÿæˆé•¿åº¦
                )
                if metrics:
                    all_step_metrics.append(metrics)



            # æ·»åŠ Gumbelå™ªå£°
            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)
            x0 = torch.argmax(logits_with_noise, dim=-1)  # b, l
            
            # é‡æ©ç ç­–ç•¥
            if remasking == 'low_confidence':
                p = F.softmax(logits, dim=-1)
                x0_p = torch.squeeze(
                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1)  # b, l
            elif remasking == 'random':
                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)
            else:
                raise NotImplementedError(f"Remasking strategy '{remasking}' not implemented")
            
            # ä»…å…è®¸åœ¨â€œå½“å‰å—â€å†…é‡‡æ ·ï¼šå—å‰ã€å—åéƒ½è®¾ä¸º -inf
            x0_p[:, : gen_start + num_block * block_length] = -np.inf
            x0_p[:, gen_start + (num_block + 1) * block_length :] = -np.inf
            
            x0 = torch.where(mask_index, x0, x)
            confidence = torch.where(mask_index, x0_p, -np.inf)
            
            # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„tokenè¿›è¡Œè½¬ç§»
            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)
            for j in range(confidence.shape[0]):
                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])
                transfer_index[j, select_index] = True
            x[transfer_index] = x0[transfer_index]


            #è®°å½•è§£ç çš„ä½ç½®ä¸»è¦æ˜¯æƒ³åšheatmap
            # è®°å½•å½“å‰æ­¥è§£ç çš„ä½ç½®
            # if trigger_analysis:
            #     #è·å–ç”ŸæˆåŒºåŸŸçš„ç½®ä¿¡åº¦ï¼ˆç›¸å¯¹äºgen_start)
            #     gen_region_confidence=confidence[0,gen_start:gen_start+gen_length].to(torch.float32).cpu().numpy()
            #     #æ›¿æ¢-infä¸ºä¸€ä¸ªåˆç†çš„æœ€å°å€¼ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
            #     gen_region_confidence=np.where(gen_region_confidence == -np.inf,np.nan,gen_region_confidence)
            #     confidence_matrix.append(gen_region_confidence)


                # ä¸å†è®°å½•æ¯æ­¥è§£ç ä½ç½®

            # --- å†…å­˜é‡Šæ”¾çš„å…³é”®æ­¥éª¤ï¼ˆåˆ æ‰å¤§é‡çš„æƒé‡ï¼‰---
            # åœ¨å¾ªç¯çš„æœ«å°¾ï¼Œæ˜¾å¼åˆ é™¤ä¸å†éœ€è¦çš„å¤§å¼ é‡
            if outputs is not None:
                del outputs
            if attentions_to_analyze is not None:
                del attentions_to_analyze
            
            # å¦‚æœä½¿ç”¨GPUï¼Œå¼ºåˆ¶æ¸…ç†PyTorchçš„ç¼“å­˜å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            current_generated_tokens = x[0][gen_start : gen_start + gen_length]
            # ä½¿ç”¨ skip_special_tokens=False æ¥çœ‹åˆ° <|mdm_mask|>
            current_text = tokenizer.decode(current_generated_tokens, skip_special_tokens=False)
            # ç›´æ¥æ‰“å°ï¼Œè®©ç»ˆç«¯è‡ªåŠ¨å¤„ç†æ¢è¡Œ
            print(current_text)
            print("-" * (len(f"--- [Block {num_block+1}, Step {i+1}/{steps}] ---"))) # åˆ†éš”çº¿

            # æ”¶é›†è§£ç æ–‡æœ¬ç”¨äºç»˜å›¾
            # if trigger_analysis:
            #     all_decoded_texts.append(current_text)
        
            # --- åœ¨ç”Ÿæˆå¾ªç¯ç»“æŸåï¼Œæ‰§è¡Œä¿å­˜å’Œç»˜å›¾ ---
        if trigger_analysis and all_step_metrics:
            print("\n[åˆ†æ] ç”Ÿæˆè¿‡ç¨‹ç»“æŸï¼Œå¼€å§‹ç”ŸæˆIEARåˆ†æå›¾...")
            if sample_idx is not None:
                base_dir = Path(f"IEAR_results_{query_position}") / f"test_{sample_idx}"
                iear_dir = base_dir / "IEAR_analysis_output"
                # heatmap_dir = base_dir / "heatmap"
            else:
                base_dir = Path(f"IEAR_results_{query_position}")
                iear_dir = base_dir / "IEAR_analysis_output"
                # heatmap_dir = base_dir / "heatmap"
            iear_dir.mkdir(parents=True, exist_ok=True)
            # heatmap_dir.mkdir(parents=True, exist_ok=True)
            plot_iear_metrics_per_step(
                all_step_metrics,
                # all_decoded_texts,
                save_dir=str(iear_dir)
            )
    
    return x


class LLaDAInference:
    """
    LLaDAæ¨ç†ç±»ï¼Œç”¨äºæµ‹è¯•åŒå‘èƒ½åŠ› - ä¼˜åŒ–ç‰ˆæœ¬
    """

    def __init__(
    self,
    model_path: str,
    device: str = "cuda",
    mask_id: int = 126336,
    max_length: int = 4096,
    use_accelerate: bool = False,
    torch_dtype: torch.dtype = torch.bfloat16,
    tokenizer: AutoTokenizer = None,
    model: AutoModel = None,
    **kwargs
    ):
        """
        åˆå§‹åŒ–LLaDAæ¨ç†ç±»ï¼Œç”¨äºæµ‹è¯•åŒå‘èƒ½åŠ›
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡
            mask_id: [MASK]çš„token id
            max_length: æœ€å¤§é•¿åº¦
            use_accelerate: æ˜¯å¦ä½¿ç”¨accelerate
            kwargs: å…¶ä»–å‚æ•°
        """
        self.model_path = model_path
        self.device = device
        self.mask_id = mask_id
        self.max_length = max_length
        self.use_accelerate = use_accelerate
        self.kwargs = kwargs
        self.tokenizer = tokenizer
        self.model = model
        self.torch_dtype = torch_dtype
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜æœºåˆ¶
        self._mask_position_cache = {}  # ç¼“å­˜maskä½ç½®ä¿¡æ¯
        self._tensor_cache = {}  # ç¼“å­˜tensorè½¬æ¢ç»“æœ

    def _get_mask_positions(self, input_ids: torch.Tensor, prompt_hash: str):
        """è·å–maskä½ç½®ä¿¡æ¯ï¼Œä½¿ç”¨ç¼“å­˜ä¼˜åŒ–"""
        if prompt_hash in self._mask_position_cache:
            return self._mask_position_cache[prompt_hash]
        
        # æ‰¾åˆ°mask tokençš„ä½ç½®
        mask_positions = (input_ids == self.mask_id).nonzero(as_tuple=True)
        if len(mask_positions[0]) == 0:
            raise ValueError("No mask tokens found in prompt")

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªmask tokençš„ä½ç½®
        first_mask_pos = mask_positions[1][0].item()
        last_mask_pos = mask_positions[1][-1].item()

        # éªŒè¯mask tokenæ˜¯è¿ç»­çš„
        expected_mask_count = last_mask_pos - first_mask_pos + 1
        actual_mask_count = len(mask_positions[1])
        if actual_mask_count != expected_mask_count:
            raise ValueError(f"Mask tokens are not continuous. Expected {expected_mask_count}, got {actual_mask_count}")
        
        result = (first_mask_pos, last_mask_pos)
        self._mask_position_cache[prompt_hash] = result
        return result

    def _process_stop_tokens(self, text: str, stop_tokens: Optional[List[str]]) -> str:
        """ä¼˜åŒ–çš„åœæ­¢tokenå¤„ç†"""
        if not stop_tokens:
            return text
        
        # æ‰¾åˆ°æœ€æ—©å‡ºç°çš„åœæ­¢token
        min_pos = len(text)
        for stop_token in stop_tokens:
            pos = text.find(stop_token)
            if pos != -1 and pos < min_pos:
                min_pos = pos
        
        if min_pos < len(text):
            return text[:min_pos]
        return text

    def generate_text(
        self,
        prompt: Union[str, List[int]],
        answer_length: int = 1024,
        sampling_steps: int = 1024,
        block_length: int = 1024,
        remask_strategy: str = "low_confidence",
        temperature: float = 0.0,
        cfg_scale: float = 0.0,
        stop_tokens: Optional[List[str]] = None,
        output_attentions: bool = False,  # ä¿ç•™è¿™ä¸ªå‚æ•°
        IEAR_analyse: bool = False,
        sample_idx:int|None=None,#æ–°å¢ï¼ŒåæœŸè¦åˆ æ‰
        query_position:int=0,#æ–°å¢ï¼ŒåæœŸè¦åˆ æ‰
    ) -> str:
        """
        é€šç”¨æ–¹æ³•ï¼šç”Ÿæˆæ–‡æœ¬ - ä¼˜åŒ–ç‰ˆæœ¬
        Args:
            prompt: æç¤ºï¼ˆå­—ç¬¦ä¸²ä¸­å·²åŒ…å«mask tokenï¼‰
            answer_length: ç­”æ¡ˆé•¿åº¦
            sampling_steps: é‡‡æ ·æ­¥æ•°
            block_length: å—é•¿åº¦
            remask_strategy: é‡æ©ç ç­–ç•¥
            temperature: æ¸©åº¦
            cfg_scale: åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç¼©æ”¾
            stop_tokens: åœæ­¢token
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        # ä¼˜åŒ–ï¼šåˆ›å»ºpromptçš„hashç”¨äºç¼“å­˜
        if isinstance(prompt, str):
            input_ids = self.tokenizer(prompt)['input_ids']
            input_ids = torch.tensor(input_ids).to(self.device).unsqueeze(0)
        else:
            input_ids = torch.tensor(prompt).to(self.device).unsqueeze(0)

        # æ‰¾åˆ°mask tokençš„ä½ç½®
        mask_positions = (input_ids == self.mask_id).nonzero(as_tuple=True)
        if len(mask_positions[0]) == 0:
            raise ValueError("No mask tokens found in prompt")

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªmask tokençš„ä½ç½®
        first_mask_pos = mask_positions[1][0].item()
        last_mask_pos = mask_positions[1][-1].item()

        # éªŒè¯mask tokenæ˜¯è¿ç»­çš„
        expected_mask_count = last_mask_pos - first_mask_pos + 1
        actual_mask_count = len(mask_positions[1])
        if actual_mask_count != expected_mask_count:
            raise ValueError(f"Mask tokens are not continuous. Expected {expected_mask_count}, got {actual_mask_count}")

        # æ‰§è¡Œç”Ÿæˆï¼ˆæ ¸å¿ƒé€»è¾‘ä¿æŒä¸å˜ï¼‰
        generated = generate(
            model=self.model,
            prompt=input_ids,
            gen_start=first_mask_pos,
            steps=sampling_steps,
            gen_length=answer_length,
            block_length=block_length,
            temperature=temperature,
            cfg_scale=cfg_scale,
            remasking=remask_strategy,
            mask_id=self.mask_id,
            output_attentions=output_attentions,  # ä¿ç•™è¿™ä¸ªå‚æ•°ï¼Œå› ä¸ºå‡½æ•°å®šä¹‰ä¸­æœ‰
            IEAR_analyse=IEAR_analyse,
            sample_idx=sample_idx,  #æ–°å¢ï¼ŒåæœŸå¾—åˆ æ‰  
            query_position=query_position  # æ–°å¢å‚æ•°
        )

        # # å¤„ç†è¿”å›ç»“æœ
        # if output_attentions:
        #     generated, all_attentions = result
        # else:
        #     generated = result
        #     all_attentions = None
        
        # ä¼˜åŒ–ï¼šç›´æ¥æå–maskå¡«å……åŒºåŸŸï¼Œå‡å°‘ç´¢å¼•æ“ä½œ
        mask_filled_tokens = generated[0][first_mask_pos:last_mask_pos+1]
        
        # ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§è§£ç ï¼Œå‡å°‘é‡å¤çš„tokenizerè°ƒç”¨
        generated_text = self.tokenizer.decode(mask_filled_tokens, skip_special_tokens=False)

        # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„åœæ­¢tokenå¤„ç†
        generated_text = self._process_stop_tokens(generated_text, stop_tokens)
        
        # ç§»é™¤ç‰¹æ®Štokenï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        generated_text = generated_text.replace("<|mdm_mask|>", "").strip()

        # # å¯é€‰ï¼šä¿å­˜æ³¨æ„åŠ›æƒé‡
        # if output_attentions and save_attentions_path:

        #     save_dir = Path(save_attentions_path)
        #     save_dir.mkdir(parents=True, exist_ok=True)
        #     torch.save(all_attentions, save_dir / "attention_weights.pt")
        #     print(f"æ³¨æ„åŠ›æƒé‡å·²ä¿å­˜åˆ°: {save_dir / 'attention_weights.pt'}")

        # # è¿”å›ç»“æœ
        # if output_attentions:
        #     return generated_text, all_attentions
        # else:

        return generated_text

   


def create_llada_inference(
    model_path: str = None,
    device: str = "cuda",
    use_accelerate: bool = False,
    tokenizer: AutoTokenizer = None,
    model: AutoModel = None,
    mask_id: int = 126336,
    max_length: int = 4096,
    torch_dtype: torch.dtype = torch.bfloat16
) -> LLaDAInference:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºLLaDAæ¨ç†å™¨
    
    Args:
        model_path: LLaDAæ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæä¾›äº†tokenizerå’Œmodelï¼Œæ­¤å‚æ•°å¯é€‰ï¼‰
        device: è®¾å¤‡ç±»å‹
        use_accelerate: æ˜¯å¦ä½¿ç”¨Accelerate
        tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨ï¼ˆå¯é€‰ï¼‰
        model: å·²åŠ è½½çš„æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        mask_id: æ©ç token ID
        max_length: æœ€å¤§é•¿åº¦
        torch_dtype: æ¨¡å‹ç²¾åº¦
        
    Returns:
        LLaDAInferenceå®ä¾‹
    """
    return LLaDAInference(
        model_path=model_path,
        device=device,
        use_accelerate=use_accelerate,
        tokenizer=tokenizer,
        model=model,
        mask_id=mask_id,
        max_length=max_length,
        torch_dtype=torch_dtype
    )



# æµ‹è¯•ç”¨ä¾‹
if __name__ == "__main__":
    # å¯¼å…¥llada_loader
    import sys
    import os
    import random
    import json
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    
    from llada_loader import load_model
    from prompt_constructor_gsm8k import GSM8KPromptConstructor
    from utils import extract_gsm8k_answer
    from gsm8k_handler_v2 import GSM8KHandler
    
    print("=== LLaDA æ‰¹é‡ACARåˆ†ææµ‹è¯• ===")
    
    #é…ç½®å‚æ•°
    N_SAMPLES = 20 #ä»æµ‹è¯•é›†éšæœºæŠ½å–çš„æ ·æœ¬æ•°
    RANDOM_SEED = 1234 #éšæœºç§å­
    random.seed(RANDOM_SEED)


    # 1. ä½¿ç”¨llada_loaderåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print("æ­¥éª¤1: ä½¿ç”¨llada_loaderåŠ è½½æ¨¡å‹...")
    model_path = "/home/share/model_weight/llada/LLaDA-8B-Base/"
    device = "cuda:3"
    
    model, tokenizer = load_model(
        model_path=model_path,
        device=device,
        use_accelerate=False,
        mask_id=126336,
        max_length=4096,
        torch_dtype=torch.bfloat16
    )
    
    print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {device}")
    print(f"åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    # 2. ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹å’Œåˆ†è¯å™¨åˆ›å»ºæ¨ç†å™¨
    print("\næ­¥éª¤2: åˆ›å»ºæ¨ç†å™¨...")
    inference = create_llada_inference(
        model_path=model_path,
        device=device,
        tokenizer=tokenizer,  # ä¼ å…¥å·²åŠ è½½çš„åˆ†è¯å™¨
        model=model,          # ä¼ å…¥å·²åŠ è½½çš„æ¨¡å‹
        mask_id=126336,
        max_length=4096,
        torch_dtype=torch.bfloat16
    )
    
    print("æ¨ç†å™¨åˆ›å»ºå®Œæˆ")
    

    # 3. åŠ è½½GSM8Kæ•°æ®é›†
    print("\næ­¥éª¤3: åŠ è½½GSM8Kæ•°æ®é›†...")
    data_handler = GSM8KHandler(data_dir="/home/share/datasets/gsm8k/")
    train_dataset, test_dataset = data_handler.prepare_for_evaluation("test", n_shots=4)

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")

    # 4. éšæœºæŠ½å–æµ‹è¯•æ ·æœ¬
    print(f"\næ­¥éª¤4: ä»æµ‹è¯•é›†éšæœºæŠ½å– {N_SAMPLES} ä¸ªæ ·æœ¬...")
    test_indices = random.sample(range(len(test_dataset)), N_SAMPLES)
    test_samples = [test_dataset[i] for i in test_indices]
    print(f"æŠ½å–çš„æµ‹è¯•æ ·æœ¬ç´¢å¼•: {test_indices[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª

    # 5. å›ºå®šä½¿ç”¨å‰4ä¸ªè®­ç»ƒæ ·æœ¬ä½œä¸ºfew-shotç¤ºä¾‹
    print("\næ­¥éª¤5: å‡†å¤‡few-shotç¤ºä¾‹...")
    train_samples = [train_dataset[i] for i in range(4)]

    for position in range(4,5):
        print(f"\n=======å¼€å§‹Position={position}çš„æµ‹è¯•=======")
        # 6. åˆ›å»ºpromptæ„é€ å™¨
        print("\næ­¥éª¤6: åˆ›å»ºpromptæ„é€ å™¨...")
        prompt_constructor = GSM8KPromptConstructor(n_shots=4, query_position=position)  # ä½¿ç”¨é»˜è®¤ä½ç½®
        
        # 7. æ‰¹é‡æ¨ç†ä¸åˆ†æ 
        print(f"\næ­¥éª¤6: å¼€å§‹æ‰¹é‡æ¨ç†ä¸ACARåˆ†æ ({N_SAMPLES} ä¸ªæ ·æœ¬)...")

        results = []

        for idx, test_sample in enumerate(test_samples, start=1):
            print(f"\n{'='*80}")
            print(f"å¤„ç†æ ·æœ¬ {idx}/{N_SAMPLES} (æµ‹è¯•é›†ç´¢å¼•: {test_indices[idx-1]})")
            print(f"{'='*80}")
            print(f"é—®é¢˜: {test_sample['question'][:200]}...")

            try:
                #æ„å»ºprompt
                prompt = prompt_constructor.construct_prompt(train_samples, test_sample, mask_length=256)

                # ä¿®æ”¹generateå‡½æ•°è°ƒç”¨ï¼Œä¼ å…¥sample_idx
                # æ³¨æ„ï¼šéœ€è¦åœ¨generateå†…éƒ¨æ ¹æ®sample_idxæ„å»ºè¾“å‡ºç›®å½•
                generated_text = inference.generate_text(
                    prompt=prompt,
                    answer_length=256,
                    sampling_steps=256,
                    block_length=256,
                    temperature=0.0,
                    stop_tokens=["Question:", "Answer:"],
                    output_attentions=True,
                    IEAR_analyse=True,
                    sample_idx=idx, #è¡¨ç¤ºè¿™æ˜¯ç¬¬å‡ ä¸ªï¼ŒåæœŸè¦åˆ æ‰
                    query_position=position #è¡¨ç¤ºä½ç½®
                )

                # æå–ç­”æ¡ˆ
                predicted_answer = extract_gsm8k_answer(generated_text)
                true_answer = extract_gsm8k_answer(test_sample['answer'])
                is_correct = (predicted_answer == true_answer)

                result = {
                    "sample_idx": idx,
                    "test_dataset_idx": test_indices[idx-1],
                    "question": test_sample['question'],
                    "predicted_answer": predicted_answer,
                    "true_answer": true_answer,
                    "is_correct": is_correct,
                    "generated_text": generated_text
                }
                results.append(result)
                print(f"é¢„æµ‹ç­”æ¡ˆ: {predicted_answer}")
                print(f"çœŸå®ç­”æ¡ˆ: {true_answer}")
                print(f"æ­£ç¡®æ€§: {'âœ“ æ­£ç¡®' if is_correct else 'âœ— é”™è¯¯'}")

            except Exception as e:
                print(f"æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                
                result = {
                    "sample_idx": idx,
                    "test_dataset_idx": test_indices[idx-1],
                    "question": test_sample['question'],
                    "error": str(e),
                    "is_correct": False
                }
                results.append(result)


        # 8. ä¿å­˜æ±‡æ€»ç»“æœ
        print(f"\n{'='*80}")
        print("æ­¥éª¤7:ä¿å­˜æ±‡æ€»ç»“æœ...")
        print(f"{'='*80}")

        # åˆ›å»ºresultsç›®å½•
        results_dir = Path(f"batch_analysis_iear_results_{position}")
        results_dir.mkdir(exist_ok=True)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(results_dir / "all_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        # è®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        total = len(results)
        correct = sum(1 for r in results if r.get("is_correct", False))
        accuracy = correct / total if total > 0 else 0
        
        summary = {
            "total_samples": total,
            "correct": correct,
            "incorrect": total - correct,
            "accuracy": accuracy,
            "random_seed": RANDOM_SEED,
            "test_indices": test_indices,
            "query_position": position
        }

        with open(results_dir / "summary.json", "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nposition={position}çš„æœ€ç»ˆç»Ÿè®¡:")
        print(f"- æ€»æ ·æœ¬æ•°: {total}")
        print(f"- æ­£ç¡®æ•°: {correct}")
        print(f"- é”™è¯¯æ•°: {total - correct}")
        print(f"- å‡†ç¡®ç‡: {accuracy:.2%}")
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_dir}/")
        print(f"- æ±‡æ€»ç»Ÿè®¡: summary.json")
        print(f"- IEARåˆ†æå›¾: test_1/ ~ test_{N_SAMPLES}/ (å„è‡ªçš„IEAR_analysis_outputç›®å½•)")
    
    print("\n=== æ‰¹é‡æµ‹è¯•å®Œæˆ ===")
