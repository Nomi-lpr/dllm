#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è„šæœ¬ï¼šä½¿ç”¨ç”Ÿæˆçš„ICDåºåˆ—è¿›è¡Œæ¨ç†å¹¶è®¡ç®—å‡†ç¡®ç‡

æ ¹æ®å‚æ•°æœç´¢å¯¹åº”çš„JSONæ–‡ä»¶ï¼Œä½¿ç”¨JSONä¸­çš„ICDåºåˆ—è¿›è¡Œæ¨ç†ï¼Œè®¡ç®—å‡†ç¡®ç‡ã€‚

ä½¿ç”¨æ–¹æ³•:
    python test_icd_sequences.py --task gsm8k --model llada --sampler random [å…¶ä»–å‚æ•°...]
"""

import os
import sys
import json
import re
import argparse
import glob
import torch
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_script_path = os.path.abspath(__file__)
project_root = os.path.dirname(current_script_path)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from transformers import AutoTokenizer
from model.model_llada import LLaDAModelLM
from utils.eval_utils import gsm8k_check, eval_gsm8k
from lever_lm.load_ds_utils import load_gsm8k_ds, load_mmlu_ds, load_ceval_cmmlu_ds
from open_mmicl.prompt_template import PromptTemplate
from open_mmicl.metrics import GSM8KMetrics, MMLUMetrics, CevalMetrics, CmmluMetrics
import hydra
from omegaconf import DictConfig


def find_json_by_params(
    search_dir: str,
    task: str,
    model: str,
    sampler: str,
    scorer: str,
    construct_order: str,
    beam_size: int,
    few_shot: int,
    candidate_num: int,
    sample_num: int,
    mc_num: Optional[int] = None,
    coarse_k: Optional[int] = None,
    mmr_lambda: Optional[float] = None,
) -> Optional[str]:
    """
    æ ¹æ®å‚æ•°æœç´¢å¯¹åº”çš„JSONæ–‡ä»¶
    
    Args:
        search_dir: æœç´¢ç›®å½•ï¼ˆé€šå¸¸æ˜¯sub_proc_dataæˆ–generated_dataï¼‰
        task: ä»»åŠ¡åç§°ï¼ˆå¦‚gsm8kï¼‰
        model: æ¨¡å‹åç§°ï¼ˆå¦‚lladaï¼‰
        sampler: é‡‡æ ·å™¨åç§°ï¼ˆå¦‚random_sampleræˆ–randomï¼‰
        scorer: è¯„åˆ†å‡½æ•°ï¼ˆå¦‚infoscoreï¼‰
        construct_order: æ„é€ é¡ºåºï¼ˆå¦‚no_orderï¼‰
        beam_size: beamå¤§å°
        few_shot: few-shotæ•°é‡
        candidate_num: å€™é€‰æ•°é‡
        sample_num: æ ·æœ¬æ•°é‡
        
    Returns:
        JSONæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å›None
    """
    # æ„å»ºæœç´¢æ¨¡å¼
    # æ–‡ä»¶åæ ¼å¼ï¼ˆæ–°ç‰ˆï¼‰:
    #   task-task-model-sampler-scorer:xxx-construct_order:xxx-beam_size:x-few_shot:x-candidate_num:x-sample_num:x-mc_num:y.json
    # - å¦‚æœæŒ‡å®š mc_numï¼šç²¾ç¡®åŒ¹é…è¯¥ mc_num
    # - å¦‚æœæœªæŒ‡å®šï¼šåœ¨ sample_num åé¢ä½¿ç”¨é€šé…ç¬¦ *.jsonï¼Œå…¼å®¹æ—§ç‰ˆï¼ˆæ—  mc_num å­—æ®µï¼‰
    # æ³¨æ„ï¼šå®é™…æ–‡ä»¶åä½¿ç”¨çš„æ˜¯ sampler_nameï¼ˆå¦‚ text_sim_qwen_mmrï¼‰ï¼Œå¯èƒ½æ²¡æœ‰ _sampler åç¼€
    
    # å…ˆå°è¯•ä½¿ç”¨åŸå§‹ sampler åç§°ï¼ˆå› ä¸º generate_data_main.py ä½¿ç”¨çš„æ˜¯ cfg.sampler.sampler_nameï¼‰
    sampler_patterns = [sampler]
    # å¦‚æœåŸå§‹åç§°ä¸­æ²¡æœ‰ _samplerï¼Œä¹Ÿå°è¯•æ·»åŠ åç¼€ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
    if "_sampler" not in sampler:
        sampler_patterns.append(f"{sampler}_sampler")
    
    # å°è¯•æ¯ä¸ª sampler_pattern
    for sampler_pattern in sampler_patterns:
        pattern = (
            f"{task}-{task}-{model}-{sampler_pattern}-scorer:{scorer}-"
            f"construct_order:{construct_order}-"
            f"beam_size:{beam_size}-few_shot:{few_shot}-"
            f"candidate_num:{candidate_num}-sample_num:{sample_num}"
        )
        
        # æ·»åŠ  mc_numï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if mc_num is not None:
            pattern = f"{pattern}-mc_num:{mc_num}"
        
        # æ·»åŠ  coarse_k å’Œ lambdaï¼ˆå¦‚æœæŒ‡å®šï¼Œç”¨äº MMLUï¼‰
        if coarse_k is not None and mmr_lambda is not None:
            pattern = f"{pattern}-coarse_k:{coarse_k}-lambda:{mmr_lambda}"
        
        # æ„å»ºå®Œæ•´æœç´¢è·¯å¾„
        # å¦‚æœ coarse_k å’Œ lambda æœªæŒ‡å®šï¼Œä½¿ç”¨é€šé…ç¬¦åŒ¹é…ï¼ˆå…¼å®¹åŒ…å«è¿™äº›å‚æ•°çš„æ–‡ä»¶åï¼‰
        if coarse_k is None or mmr_lambda is None:
            # å…è®¸ sample_num åé¢è¿½åŠ ä»»æ„åç¼€ï¼ˆä¾‹å¦‚ -mc_num:128-coarse_k:200-lambda:0.1ï¼‰
            pattern = f"{pattern}*.json"
        else:
            pattern = f"{pattern}.json"
        
        # åœ¨å½“å‰æœç´¢ç›®å½•ä¸­æŸ¥æ‰¾
        search_path = os.path.join(search_dir, pattern)
        matches = glob.glob(search_path)
        if matches:
            return matches[0]
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•åœ¨generated_dataç›®å½•ä¸­æœç´¢ï¼ˆå¦‚æœå½“å‰åœ¨sub_proc_dataä¸­ï¼‰
        if "sub_proc_data" in search_dir:
            parent_dir = os.path.dirname(search_dir)  # è·å– .../generated_data
            generated_data_dir = parent_dir
            if os.path.exists(generated_data_dir):
                search_path = os.path.join(generated_data_dir, pattern)
                matches = glob.glob(search_path)
                if matches:
                    return matches[0]
        
        # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•åœ¨generated_dataç›®å½•ä¸­æœç´¢ï¼ˆå¦‚æœå½“å‰ä¸åœ¨sub_proc_dataä¸­ï¼‰
        if "sub_proc_data" not in search_dir and "generated_data" in search_dir:
            # å·²ç»åœ¨generated_dataä¸­ï¼Œä¸éœ€è¦å†æœç´¢
            pass
        elif "generated_data" not in search_dir:
            # å¦‚æœä¸åœ¨generated_dataä¸­ï¼Œå°è¯•æ·»åŠ generated_dataè·¯å¾„
            if os.path.exists(os.path.join(search_dir, "generated_data")):
                generated_data_dir = os.path.join(search_dir, "generated_data")
                search_path = os.path.join(generated_data_dir, pattern)
                matches = glob.glob(search_path)
                if matches:
                    return matches[0]
    
    return None


def load_icd_data(json_file: str) -> Dict:
    """
    åŠ è½½ICDåºåˆ—æ•°æ®
    
    Args:
        json_file: JSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        ICDæ•°æ®å­—å…¸ {anchor_id: {id_list: [...], score_list: [...]}}
    """
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def get_best_icd_sequence_with_query_position(
    icd_data: Dict,
    anchor_id: str,
    icd_rank: int = 0,
) -> Tuple[List[int], int]:
    """
    è·å–ç¬¬ icd_rank åçš„ ICD åºåˆ—ï¼ˆæŒ‰ score ä»é«˜åˆ°ä½æ’åºï¼‰å’Œ query åœ¨åºåˆ—ä¸­çš„ä½ç½®
    
    Args:
        icd_data: ICDæ•°æ®
        anchor_id: anchor ID
        
    Returns:
        (icd_sequence, query_position)
        icd_sequence: å®Œæ•´åºåˆ—ï¼ˆåŒ…å«anchorï¼‰
        query_position: anchoråœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼ˆ0-basedï¼‰
    """
    if anchor_id not in icd_data:
        raise ValueError(f"Anchor {anchor_id} not found in ICD data")
    
    anchor_data = icd_data[anchor_id]
    id_list = anchor_data['id_list']
    score_list = anchor_data['score_list']

    if icd_rank < 0:
        raise ValueError(f"icd_rank must be >= 0, got {icd_rank}")
    if len(id_list) == 0:
        raise ValueError(f"Anchor {anchor_id} has empty id_list")
    if len(score_list) != len(id_list):
        raise ValueError(
            f"Anchor {anchor_id} has mismatched lengths: "
            f"len(id_list)={len(id_list)} vs len(score_list)={len(score_list)}"
        )

    # æŒ‰ score ä»é«˜åˆ°ä½æ’åºï¼Œé€‰ç¬¬ icd_rank åï¼ˆ0 è¡¨ç¤ºæœ€é«˜åˆ†ï¼‰
    ranked = sorted(range(len(score_list)), key=lambda i: score_list[i], reverse=True)
    if icd_rank >= len(ranked):
        raise ValueError(
            f"icd_rank={icd_rank} out of range for anchor {anchor_id}: "
            f"only {len(ranked)} candidates"
        )
    chosen_idx = ranked[icd_rank]
    best_sequence = id_list[chosen_idx]
    
    # æ‰¾åˆ°anchoråœ¨åºåˆ—ä¸­çš„ä½ç½®
    anchor_id_int = int(anchor_id)
    if anchor_id_int not in best_sequence:
        raise ValueError(f"Anchor {anchor_id} not found in best sequence {best_sequence}")
    
    query_position = best_sequence.index(anchor_id_int)
    
    return best_sequence, query_position


def build_prompt_from_template(
    icd_samples: List[Dict],
    query_sample: Dict,
    query_position: int,
    prompt_template: Optional[PromptTemplate] = None,
    split_token: str = "\n\n",
) -> str:
    """
    ä½¿ç”¨ PromptTemplate æ„å»º promptï¼ˆæ”¯æŒ GSM8K å’Œ MMLUï¼‰
    
    Args:
        icd_samples: ICDæ ·æœ¬åˆ—è¡¨ï¼ˆè®­ç»ƒæ ·æœ¬ï¼‰
        query_sample: queryæ ·æœ¬ï¼ˆanchoræ ·æœ¬ï¼‰
        query_position: queryåœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼ˆ0è¡¨ç¤ºæœ€å‰ï¼Œnshotè¡¨ç¤ºæœ€åï¼‰
        prompt_template: PromptTemplate å®ä¾‹ï¼ˆå¿…é¡»æä¾›ï¼‰
        split_token: åˆ†éš”ç¬¦ï¼ˆé»˜è®¤ "\n\n"ï¼‰
        
    Returns:
        æ„å»ºå¥½çš„promptå­—ç¬¦ä¸²
    """
    if prompt_template is None:
        raise ValueError("prompt_template must be provided. Please ensure task config has template and column_token_map.")
    
    # ä½¿ç”¨ PromptTemplate æ„å»º prompt
    # 1. ç”Ÿæˆ ICD prompts
    icd_prompts = []
    for sample in icd_samples:
        icd_prompt = prompt_template.generate_ice_item(sample)
        icd_prompts.append(icd_prompt)
    
    # 2. ç”Ÿæˆ query promptï¼ˆä½¿ç”¨ maskï¼‰
    query_prompt = prompt_template.generate_query_item(query_sample, use_mask=True)
    
    # 3. æ ¹æ® query_position ç»„åˆ
    if query_position <= 0:
        # query åœ¨æœ€å‰
        all_prompts = [query_prompt] + icd_prompts
    elif query_position >= len(icd_prompts):
        # query åœ¨æœ€å
        all_prompts = icd_prompts + [query_prompt]
    else:
        # query åœ¨ä¸­é—´
        insert_pos = query_position
        all_prompts = icd_prompts[:insert_pos] + [query_prompt] + icd_prompts[insert_pos:]
    
    # 4. ç»„åˆæ‰€æœ‰ prompts
    combined_prompt = split_token.join(all_prompts)
    return combined_prompt


def test_icd_sequences(
    # æœç´¢JSONçš„å‚æ•°
    task: str = "gsm8k",
    model: str = "llada",
    sampler: str = "random",
    scorer: str = "infoscore",
    construct_order: str = "no_order",
    beam_size: int = 3,
    few_shot: int = 4,
    candidate_num: int = 10,
    sample_num: int = 10,
    mc_num: Optional[int] = None,
    coarse_k: Optional[int] = None,  # MMLU å‚æ•°
    mmr_lambda: Optional[float] = None,  # MMLU å‚æ•°
    
    # è¯„æµ‹å‚æ•°ï¼ˆå¯ä»configè¯»å–é»˜è®¤å€¼ï¼‰
    config_path: str = "./configs",
    config_name: str = "generate_data.yaml",
    model_path: Optional[str] = None,
    device: str = "cuda:0",
    mask_length: Optional[int] = None,
    mask_id: Optional[int] = None,
    block_length: Optional[int] = None,
    gen_length: Optional[int] = None,
    steps: Optional[int] = None,
    temperature: Optional[float] = None,
    mode: str = "original",
    icd_rank: int = 0,
) -> Dict:
    """
    æµ‹è¯•ICDåºåˆ—çš„å‡†ç¡®ç‡
    
    Args:
        task: ä»»åŠ¡åç§°
        model: æ¨¡å‹åç§°
        sampler: é‡‡æ ·å™¨åç§°
        scorer: è¯„åˆ†å‡½æ•°
        construct_order: æ„é€ é¡ºåº
        beam_size: beamå¤§å°
        few_shot: few-shotæ•°é‡
        candidate_num: å€™é€‰æ•°é‡
        sample_num: æ ·æœ¬æ•°é‡
        mc_num: ç”Ÿæˆæ—¶ä½¿ç”¨çš„ Monte Carlo é‡‡æ ·æ¬¡æ•°ï¼ˆç”¨äºç²¾ç¡®åŒ¹é…ç‰¹å®šç»“æœæ–‡ä»¶ï¼Œå¯é€‰ï¼‰
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        config_name: é…ç½®æ–‡ä»¶åç§°
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œä»configè¯»å–ï¼‰
        device: è®¾å¤‡
        mask_length: maské•¿åº¦ï¼ˆå¦‚æœä¸ºNoneï¼Œä»configè¯»å–ï¼‰
        mask_id: mask token IDï¼ˆå¦‚æœä¸ºNoneï¼Œä»configè¯»å–ï¼‰
        block_length: å—é•¿åº¦ï¼ˆå¦‚æœä¸ºNoneï¼Œä»configè¯»å–ï¼‰
        gen_length: ç”Ÿæˆé•¿åº¦ï¼ˆå¦‚æœä¸ºNoneï¼Œä»configè¯»å–ï¼‰
        steps: é‡‡æ ·æ­¥æ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œä»configè¯»å–ï¼‰
        temperature: æ¸©åº¦ï¼ˆå¦‚æœä¸ºNoneï¼Œä»configè¯»å–ï¼‰
        mode: ç”Ÿæˆæ¨¡å¼
        
    Returns:
        æµ‹è¯•ç»“æœå­—å…¸
    """
    print("="*80)
    print("ICDåºåˆ—æµ‹è¯•è„šæœ¬")
    print("="*80)
    
    # 1. åŠ è½½é…ç½®ï¼ˆæŒ‰ task æ¨å¯¼ overridesï¼Œä½¿ cfg ä¸ stage1 ä¸€è‡´ï¼‰
    print(f"\nâš™ï¸  åŠ è½½é…ç½®...")
    _task_to_dataset = {"ceval": "c-eval", "cmmlu": "c-mmlu", "mmlu": "mmlu", "gsm8k": "gsm8k"}
    overrides = [f"task={task}", f"dataset={_task_to_dataset.get(task, task)}"]
    with hydra.initialize(config_path=config_path, version_base=None):
        cfg = hydra.compose(config_name=config_name, overrides=overrides)
    
    # ä»configè¯»å–é»˜è®¤å€¼
    if model_path is None:
        model_path = cfg.infer_model.get("model_path")
        if model_path is None:
            raise ValueError("model_path must be provided via --model_path or in configs/infer_model/llada.yaml")
    
    if mask_id is None:
        mask_id = cfg.infer_model.get("mask_id")
        if mask_id is None:
            raise ValueError("mask_id must be provided via --mask_id or in configs/infer_model/llada.yaml")
    
    # ä» task.gen_args è¯»å–å‚æ•°ï¼ˆå¦‚æœå‘½ä»¤è¡ŒæœªæŒ‡å®šï¼‰
    task_gen_args = cfg.task.get("gen_args", None)
    if task_gen_args is None:
        raise ValueError(f"task.gen_args not found in configs/task/{cfg.task.task_name}.yaml")
    
    # mask_length: å‘½ä»¤è¡Œ > task.gen_args
    if mask_length is None:
        if "mask_length" not in task_gen_args:
            raise ValueError(f"mask_length not found in task.gen_args. Please set it in configs/task/{cfg.task.task_name}.yaml or via --mask_length")
        mask_length = int(task_gen_args.mask_length)
    
    # block_length: å‘½ä»¤è¡Œ > task.gen_args
    if block_length is None:
        if "block_length" not in task_gen_args:
            raise ValueError(f"block_length not found in task.gen_args. Please set it in configs/task/{cfg.task.task_name}.yaml or via --block_length")
        block_length = int(task_gen_args.block_length)
    
    # gen_length: å‘½ä»¤è¡Œ > task.gen_args
    if gen_length is None:
        if "gen_length" not in task_gen_args:
            raise ValueError(f"gen_length not found in task.gen_args. Please set it in configs/task/{cfg.task.task_name}.yaml or via --gen_length")
        gen_length = int(task_gen_args.gen_length)
    
    # steps: å‘½ä»¤è¡Œ > task.gen_args
    if steps is None:
        if "steps" not in task_gen_args:
            raise ValueError(f"steps not found in task.gen_args. Please set it in configs/task/{cfg.task.task_name}.yaml or via --steps")
        steps = int(task_gen_args.steps)
    
    # temperature: å‘½ä»¤è¡Œ > task.gen_args (å¯é€‰ï¼Œæœ‰é»˜è®¤å€¼)
    if temperature is None:
        if "temperature" in task_gen_args:
            temperature = float(task_gen_args.temperature)
        else:
            temperature = 0.0  # é»˜è®¤å€¼
    
    print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   mask_length: {mask_length}")
    print(f"   mask_id: {mask_id}")
    # æ ¹æ®ä»»åŠ¡ç±»å‹æ˜¾ç¤ºæ¨¡æ¿æ ¼å¼
    if task == "gsm8k":
        print(f"   ä½¿ç”¨æ ¼å¼: question: <Q>\\n<answer>\\n<A>\\n</answer> (GSM8Kæ ¼å¼)")
    elif task in ("mmlu", "ceval", "cmmlu"):
        template_str = cfg.task.get("template", "")
        print(f"   ä½¿ç”¨æ ¼å¼: {template_str} (ä½¿ç”¨ PromptTemplate)")
    else:
        print(f"   ä½¿ç”¨æ ¼å¼: æ ¹æ® task.column_token_map åŠ¨æ€ç”Ÿæˆ")
    print(f"   ICD Rank: {icd_rank} (0è¡¨ç¤ºæœ€é«˜åˆ†ï¼Œ2è¡¨ç¤ºç¬¬ä¸‰å)")
    
    # 2. æœç´¢JSONæ–‡ä»¶
    print(f"\nğŸ“‚ æœç´¢JSONæ–‡ä»¶...")
    # ä¼˜å…ˆåœ¨generated_dataç›®å½•ä¸­æœç´¢
    base_dir = os.path.join(cfg.get("output_dir", "./generated_icd_data"), "generated_data")
    search_dir = base_dir  # é¦–å…ˆåœ¨generated_dataä¸­æœç´¢
    
    json_file = find_json_by_params(
        search_dir=search_dir,
        task=task,
        model=model,
        sampler=sampler,
        scorer=scorer,
        construct_order=construct_order,
        beam_size=beam_size,
        few_shot=few_shot,
        candidate_num=candidate_num,
        sample_num=sample_num,
        mc_num=mc_num,
        coarse_k=coarse_k,
        mmr_lambda=mmr_lambda,
    )
    
    if json_file is None:
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ°åŒ¹é…çš„JSONæ–‡ä»¶ã€‚æœç´¢ç›®å½•: {search_dir}\n"
            f"å‚æ•°: task={task}, model={model}, sampler={sampler}, "
            f"scorer={scorer}, construct_order={construct_order}, "
            f"beam_size={beam_size}, few_shot={few_shot}, "
            f"candidate_num={candidate_num}, sample_num={sample_num}"
        )
    
    print(f"   âœ… æ‰¾åˆ°JSONæ–‡ä»¶: {json_file}")
    
    # 3. åŠ è½½ICDæ•°æ®
    print(f"\nğŸ“‹ åŠ è½½ICDæ•°æ®...")
    icd_data = load_icd_data(json_file)
    anchor_ids = list(icd_data.keys())
    print(f"   æ‰¾åˆ° {len(anchor_ids)} ä¸ªanchoræ ·æœ¬")
    
    # 4. åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“Š åŠ è½½æ•°æ®é›†...")
    if task == "gsm8k":
        train_ds = load_gsm8k_ds(
            version=cfg.dataset.version,
            data_path=cfg.dataset.train_path,
            split="train"
        )
    elif task == "mmlu":
        train_ds = load_mmlu_ds(
            version=cfg.dataset.version,
            data_path=cfg.dataset.train_path,
            split="train"
        )
    elif task in ("ceval", "cmmlu"):
        train_ds = load_ceval_cmmlu_ds(
            version=cfg.dataset.version,
            data_path=cfg.dataset.train_path,
            split="train",
        )
    else:
        raise ValueError(f"Unsupported task: {task}")
    print(f"   è®­ç»ƒé›†å¤§å°: {len(train_ds)}")
    
    # 5. åŠ è½½æ¨¡å‹
    print(f"\nğŸ¤– åŠ è½½æ¨¡å‹...")
    print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   è®¾å¤‡: {device}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    
    # ä½¿ç”¨æœ¬åœ° LLaDAModelLMï¼ˆå« all_tied_weights_keys å…¼å®¹ï¼‰ï¼Œä¸ generate_data_main ä¸€è‡´ï¼Œé¿å… transformers ç‰ˆæœ¬å·®å¼‚æŠ¥é”™
    model = LLaDAModelLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True,
        torch_dtype=torch.bfloat16,
    )
    model.to(device)
    model.eval()
    print("   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 5.5. åˆå§‹åŒ– PromptTemplateï¼ˆå¦‚æœä»»åŠ¡éœ€è¦ï¼‰
    prompt_template_obj = None
    if task in ["gsm8k", "mmlu", "ceval", "cmmlu"]:
        # ä»é…ç½®ä¸­è¯»å– prompt ç›¸å…³å‚æ•°
        prompt_template_str = cfg.task.get("template", None)
        column_token_map = cfg.task.get("column_token_map", None)
        if column_token_map is not None:
            column_token_map = dict(column_token_map)
        mask_column_token_map = cfg.task.get("mask_column_token_map", None)
        if isinstance(mask_column_token_map, dict):
            mask_column_token_map = dict(mask_column_token_map)
        split_token = cfg.task.get("split_token", "\n\n")
        
        if prompt_template_str and column_token_map:
            prompt_template_obj = PromptTemplate(
                prompt_template=prompt_template_str,
                mask_token_str="<|mdm_mask|>",
                mask_length=mask_length,
                column_token_map=column_token_map,
                mask_column_token_map=mask_column_token_map,
            )
            print(f"   âœ… PromptTemplate åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨æ¨¡æ¿å’Œ column_token_mapï¼‰")
        else:
            print(f"   âš ï¸  Warning: æœªæä¾› template æˆ– column_token_mapï¼Œå°†ä½¿ç”¨æ—§é€»è¾‘")
    
    # 6. å‡†å¤‡æµ‹è¯•æ ·æœ¬
    print(f"\nğŸ¯ å‡†å¤‡æµ‹è¯•æ ·æœ¬...")
    test_samples = []
    for anchor_id in anchor_ids:
        anchor_idx = int(anchor_id)
        if anchor_idx < len(train_ds):
            anchor_sample = train_ds[anchor_idx].copy()  # å¤åˆ¶ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            
            # æå– answerï¼ˆç”¨äºè¯„ä¼°ï¼‰
            if task == "gsm8k":
                # GSM8K: answer å­—æ®µåŒ…å«å®Œæ•´çš„è§£ç­”è¿‡ç¨‹ï¼ˆåŒ…æ‹¬####æ ¼å¼ï¼‰
                answer = anchor_sample.get('answer', '')
            elif task in ("mmlu", "ceval", "cmmlu"):
                # MMLU/C-Eval/C-MMLU: answer å­—æ®µå·²ç»æ˜¯ "A"/"B"/"C"/"D"
                answer = anchor_sample.get('answer', '')
            else:
                answer = anchor_sample.get('answer', '')
            
            test_samples.append({
                'idx': anchor_idx,
                'answer': answer,  # ç”¨äºè¯„ä¼°çš„ç­”æ¡ˆ
                'anchor_id': anchor_id,
                'sample': anchor_sample  # ä¿å­˜å®Œæ•´æ ·æœ¬ç”¨äºæ„å»ºprompt
            })
        else:
            print(f"   âš ï¸  Warning: anchor_id {anchor_id} è¶…å‡ºè®­ç»ƒé›†èŒƒå›´")
    
    print(f"   å‡†å¤‡æµ‹è¯• {len(test_samples)} ä¸ªæ ·æœ¬")
    
    # 7. å¯¹æ¯ä¸ªanchorè¿›è¡Œæ¨ç†
    print(f"\nğŸš€ å¼€å§‹æ¨ç†...")
    results = []
    split_token = cfg.task.get("split_token", "\n\n") if task in ["gsm8k", "mmlu", "ceval", "cmmlu"] else "\n\n"
    
    for test_sample in tqdm(test_samples, desc="æ¨ç†è¿›åº¦"):
        anchor_id = test_sample['anchor_id']
        
        try:
            # è·å–æœ€ä½³ICDåºåˆ—å’Œqueryä½ç½®
            icd_sequence, query_pos_in_sequence = get_best_icd_sequence_with_query_position(
                icd_data, anchor_id, icd_rank=icd_rank
            )
            
            # åŠ è½½ICDæ ·æœ¬ï¼ˆæ’é™¤anchoræœ¬èº«ï¼‰
            anchor_id_int = int(anchor_id)
            icd_indices = [idx for idx in icd_sequence if idx != anchor_id_int]
            icd_samples = [train_ds[idx] for idx in icd_indices if idx < len(train_ds)]
            
            # è®¡ç®—queryåœ¨few-shotä¸­çš„ä½ç½®
            # query_pos_in_sequenceæ˜¯anchoråœ¨å®Œæ•´åºåˆ—ä¸­çš„ä½ç½®
            # æˆ‘ä»¬éœ€è¦è®¡ç®—å®ƒåœ¨few-shotç¤ºä¾‹ä¸­çš„ä½ç½®ï¼ˆæ’é™¤anchoråï¼‰
            query_position = sum(1 for idx in icd_sequence[:query_pos_in_sequence] if idx != anchor_id_int)
            
            # æ„å»ºpromptï¼ˆä½¿ç”¨ PromptTemplate æˆ–æ—§é€»è¾‘ï¼‰
            prompt_text = build_prompt_from_template(
                icd_samples=icd_samples,
                query_sample=test_sample['sample'],
                query_position=query_position,
                prompt_template=prompt_template_obj,
                split_token=split_token,
            )
            
            # æ‰“å°promptä¿¡æ¯
            print(f"\n{'='*80}")
            print(f"Anchor ID: {anchor_id}")
            print(f"Query Position: {query_position}")
            print(f"ICD Rank: {icd_rank}")
            print(f"ICD Sequence: {icd_sequence}")
            print(f"ICD Indices (excluding anchor): {icd_indices}")
            print(f"{'='*80}")
            print("PROMPT:")
            print(f"{'='*80}")
            print(prompt_text)
            print(f"{'='*80}\n")
            
            # Tokenize prompt
            prompt_tokens = tokenizer(prompt_text, return_tensors='pt')['input_ids'].to(device)
            
            # æ‰¾åˆ°mask tokenä½ç½®
            mask_positions = (prompt_tokens == mask_id).nonzero(as_tuple=True)
            if len(mask_positions[0]) == 0:
                raise ValueError("No mask tokens found in prompt")
            
            first_mask_pos = mask_positions[1][0].item()
            last_mask_pos = mask_positions[1][-1].item()
            
            # è°ƒç”¨src/generate.pyä¸­çš„generateå‡½æ•°
            from src.generate import generate as generate_core
            generated_tokens = generate_core(
                model=model,
                prompt=prompt_tokens,
                gen_start=first_mask_pos,
                steps=steps,
                gen_length=gen_length,
                block_length=block_length,
                temperature=temperature,
                cfg_scale=0.0,
                remasking='low_confidence',
                mask_id=mask_id
            )
            
            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆåªè§£ç maskéƒ¨åˆ†ï¼‰
            generated_text = tokenizer.batch_decode(
                generated_tokens[:, first_mask_pos:last_mask_pos+1],
                skip_special_tokens=False
            )[0]
            answer = generated_text
            
            print(f"Generated Answer: {answer}\n")
            results.append(answer)
            
        except Exception as e:
            print(f"\n   âŒ å¤„ç†anchor {anchor_id} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            results.append("")  # æ·»åŠ ç©ºç­”æ¡ˆ
    
    # 8. è¯„ä¼°å‡†ç¡®ç‡
    print(f"\nğŸ“ˆ è®¡ç®—å‡†ç¡®ç‡...")
    
    if task == "gsm8k":
        # GSM8K: ä½¿ç”¨ eval_gsm8k
        eval_dataset = [{'answer': s['answer']} for s in test_samples]
        
        class Args:
            def __init__(self):
                self.task = task
                self.model_name = model_path
                self.device = device
                self.gen_length = gen_length
                self.steps = steps
                self.block_length = block_length
                self.temperature = temperature
                self.mode = mode
                self.nshot = few_shot
                self.query_position = 0
                self.icd_rank = icd_rank
                self.sampler = sampler
                self.scorer = scorer
                self.construct_order = construct_order
                self.beam_size = beam_size
                self.candidate_num = candidate_num
                self.sample_num = sample_num
        
        args = Args()
        result_path = os.path.join(os.path.dirname(json_file), "test_results")
        
        accuracy = eval_gsm8k(
            results=results,
            dataset=eval_dataset,
            result_path=result_path,
            args=args,
            position=0,
            iswrite=True
        )
    elif task == "mmlu":
        metrics = MMLUMetrics()
        ground_truths = [s['answer'] for s in test_samples]
        batch_result = metrics.evaluate_batch(results, ground_truths)
        accuracy = batch_result['accuracy']
        result_path = os.path.join(os.path.dirname(json_file), "test_results")
        os.makedirs(result_path, exist_ok=True)
        result_file = os.path.join(result_path, f"{task}_test_results.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({
                'accuracy': accuracy,
                'correct_count': batch_result['correct_count'],
                'total_count': batch_result['total_count'],
                'results': batch_result['results'],
                'generated_texts': results,
            }, f, ensure_ascii=False, indent=2)
        print(f"   ç»“æœä¿å­˜åˆ°: {result_file}")
    elif task == "ceval":
        metrics = CevalMetrics()
        ground_truths = [s['answer'] for s in test_samples]
        batch_result = metrics.evaluate_batch(results, ground_truths)
        accuracy = batch_result['accuracy']
        result_path = os.path.join(os.path.dirname(json_file), "test_results")
        os.makedirs(result_path, exist_ok=True)
        result_file = os.path.join(result_path, f"{task}_test_results.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({
                'accuracy': accuracy,
                'correct_count': batch_result['correct_count'],
                'total_count': batch_result['total_count'],
                'results': batch_result['results'],
                'generated_texts': results,
            }, f, ensure_ascii=False, indent=2)
        print(f"   ç»“æœä¿å­˜åˆ°: {result_file}")
    elif task == "cmmlu":
        metrics = CmmluMetrics()
        ground_truths = [s['answer'] for s in test_samples]
        batch_result = metrics.evaluate_batch(results, ground_truths)
        accuracy = batch_result['accuracy']
        result_path = os.path.join(os.path.dirname(json_file), "test_results")
        os.makedirs(result_path, exist_ok=True)
        result_file = os.path.join(result_path, f"{task}_test_results.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({
                'accuracy': accuracy,
                'correct_count': batch_result['correct_count'],
                'total_count': batch_result['total_count'],
                'results': batch_result['results'],
                'generated_texts': results,
            }, f, ensure_ascii=False, indent=2)
        print(f"   ç»“æœä¿å­˜åˆ°: {result_file}")
    else:
        raise ValueError(f"Unsupported task for evaluation: {task}")
    
    # 9. æ±‡æ€»ç»“æœ
    print("\n" + "="*80)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*80)
    print(f"JSONæ–‡ä»¶: {json_file}")
    print(f"ä»»åŠ¡: {task}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_samples)}")
    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"ç»“æœä¿å­˜åˆ°: {result_path}")
    print("="*80)
    
    return {
        'json_file': json_file,
        'task': task,
        'num_samples': len(test_samples),
        'accuracy': accuracy,
        'results': results
    }


def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•ICDåºåˆ—çš„å‡†ç¡®ç‡")
    
    # æœç´¢JSONçš„å‚æ•°
    parser.add_argument('--task', type=str, default='gsm8k', help='ä»»åŠ¡åç§°')
    parser.add_argument('--model', type=str, default='llada', help='æ¨¡å‹åç§°')
    parser.add_argument('--sampler', type=str, default='random', help='é‡‡æ ·å™¨åç§°')
    parser.add_argument('--scorer', type=str, default='infoscore', help='è¯„åˆ†å‡½æ•°')
    parser.add_argument('--construct_order', type=str, default='no_order', help='æ„é€ é¡ºåº')
    parser.add_argument('--beam_size', type=int, default=3, help='beamå¤§å°')
    parser.add_argument('--few_shot', type=int, default=4, help='few-shotæ•°é‡')
    parser.add_argument('--candidate_num', type=int, default=10, help='å€™é€‰æ•°é‡')
    parser.add_argument('--sample_num', type=int, default=10, help='æ ·æœ¬æ•°é‡')
    parser.add_argument('--icd_rank', type=int, default=0, help='é€‰æ‹©ç¬¬å‡ åscoreçš„ICDåºåˆ—ï¼ˆ0è¡¨ç¤ºæœ€é«˜åˆ†ï¼‰')
    parser.add_argument('--mc_num', type=int, default=None, help='ç”Ÿæˆæ—¶ä½¿ç”¨çš„ Monte Carlo é‡‡æ ·æ¬¡æ•°ï¼ˆç”¨äºç²¾ç¡®åŒ¹é…ç‰¹å®šç»“æœæ–‡ä»¶ï¼Œå¯é€‰ï¼‰')
    parser.add_argument('--coarse_k', type=int, default=None, help='MMLU ç²—ç­›æ•°é‡ï¼ˆç”¨äºç²¾ç¡®åŒ¹é…ç‰¹å®šç»“æœæ–‡ä»¶ï¼Œå¯é€‰ï¼‰')
    parser.add_argument('--mmr_lambda', type=float, default=None, help='MMLU MMR lambda å‚æ•°ï¼ˆç”¨äºç²¾ç¡®åŒ¹é…ç‰¹å®šç»“æœæ–‡ä»¶ï¼Œå¯é€‰ï¼‰')
    
    # è¯„æµ‹å‚æ•°ï¼ˆå¯é€‰ï¼Œä¼šä»configè¯»å–é»˜è®¤å€¼ï¼‰
    parser.add_argument('--model_path', type=str, default=None, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    parser.add_argument('--mask_length', type=int, default=None, help='maské•¿åº¦')
    parser.add_argument('--mask_id', type=int, default=None, help='mask token ID')
    parser.add_argument('--block_length', type=int, default=None, help='å—é•¿åº¦')
    parser.add_argument('--gen_length', type=int, default=None, help='ç”Ÿæˆé•¿åº¦')
    parser.add_argument('--steps', type=int, default=None, help='é‡‡æ ·æ­¥æ•°')
    parser.add_argument('--temperature', type=float, default=None, help='æ¸©åº¦')
    parser.add_argument('--mode', type=str, default='original', help='ç”Ÿæˆæ¨¡å¼')
    
    args = parser.parse_args()
    
    # è¿è¡Œæµ‹è¯•
    test_icd_sequences(
        task=args.task,
        model=args.model,
        sampler=args.sampler,
        scorer=args.scorer,
        construct_order=args.construct_order,
        beam_size=args.beam_size,
        few_shot=args.few_shot,
        candidate_num=args.candidate_num,
        sample_num=args.sample_num,
        mc_num=args.mc_num,
        coarse_k=args.coarse_k,
        mmr_lambda=args.mmr_lambda,
        model_path=args.model_path,
        device=args.device,
        mask_length=args.mask_length,
        mask_id=args.mask_id,
        block_length=args.block_length,
        gen_length=args.gen_length,
        steps=args.steps,
        temperature=args.temperature,
        mode=args.mode,
        icd_rank=args.icd_rank
    )


if __name__ == "__main__":
    main()
